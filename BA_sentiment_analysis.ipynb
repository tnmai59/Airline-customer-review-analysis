{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- From the 2 dataset: review_text_data.csv and review_data_stats.csv, combine to create a new dataset which include customer reviews and recommendation. I use recommendation as a metric to consider whether a review is a positive or a negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My family and I have flown mostly on British A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This has been by far the worst service I have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In Nov 2022 I booked and paid for a return jou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BA is not treating its premium economy passeng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24 hours before our departure on BA059 to Cape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>3593</td>\n",
       "      <td>LHR-JFK-LAX-LHR. Check in was ok apart from be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>3594</td>\n",
       "      <td>LHR to HAM. Purser addresses all club passenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>3595</td>\n",
       "      <td>My son who had worked for British Airways urge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>3596</td>\n",
       "      <td>London City-New York JFK via Shannon on A318 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>3597</td>\n",
       "      <td>SIN-LHR BA12 B747-436 First Class. Old aircraf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3598 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index                                             Review\n",
       "0         0  My family and I have flown mostly on British A...\n",
       "1         1  This has been by far the worst service I have ...\n",
       "2         2  In Nov 2022 I booked and paid for a return jou...\n",
       "3         3  BA is not treating its premium economy passeng...\n",
       "4         4  24 hours before our departure on BA059 to Cape...\n",
       "...     ...                                                ...\n",
       "3593   3593  LHR-JFK-LAX-LHR. Check in was ok apart from be...\n",
       "3594   3594  LHR to HAM. Purser addresses all club passenge...\n",
       "3595   3595  My son who had worked for British Airways urge...\n",
       "3596   3596  London City-New York JFK via Shannon on A318 b...\n",
       "3597   3597  SIN-LHR BA12 B747-436 First Class. Old aircraf...\n",
       "\n",
       "[3598 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('C:/Users/HP/OneDrive - National Economics University/Data Prep/Class code/review_text_data.csv')\n",
    "df1 = pd.DataFrame(df1[\"Review\"])\n",
    "df1 = df1.reset_index().rename(columns={'index': 'Index'})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('C:/Users/HP/OneDrive - National Economics University/Data Prep/Class code/review_stats_data.csv')\n",
    "df2 = pd.DataFrame(df2[\"Recommended\"])\n",
    "df2 = df2.reset_index().rename(columns={'index': 'Index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Review</th>\n",
       "      <th>Recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My family and I have flown mostly on British A...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This has been by far the worst service I have ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>In Nov 2022 I booked and paid for a return jou...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BA is not treating its premium economy passeng...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24 hours before our departure on BA059 to Cape...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>3593</td>\n",
       "      <td>LHR-JFK-LAX-LHR. Check in was ok apart from be...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>3594</td>\n",
       "      <td>LHR to HAM. Purser addresses all club passenge...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>3595</td>\n",
       "      <td>My son who had worked for British Airways urge...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>3596</td>\n",
       "      <td>London City-New York JFK via Shannon on A318 b...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>3597</td>\n",
       "      <td>SIN-LHR BA12 B747-436 First Class. Old aircraf...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3598 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index                                             Review Recommended\n",
       "0         0  My family and I have flown mostly on British A...          no\n",
       "1         1  This has been by far the worst service I have ...          no\n",
       "2         2  In Nov 2022 I booked and paid for a return jou...          no\n",
       "3         3  BA is not treating its premium economy passeng...          no\n",
       "4         4  24 hours before our departure on BA059 to Cape...          no\n",
       "...     ...                                                ...         ...\n",
       "3593   3593  LHR-JFK-LAX-LHR. Check in was ok apart from be...          no\n",
       "3594   3594  LHR to HAM. Purser addresses all club passenge...         yes\n",
       "3595   3595  My son who had worked for British Airways urge...         yes\n",
       "3596   3596  London City-New York JFK via Shannon on A318 b...          no\n",
       "3597   3597  SIN-LHR BA12 B747-436 First Class. Old aircraf...          no\n",
       "\n",
       "[3598 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df1.merge(df2, how='right')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop(\"Index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Recommended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I have flown mostly on British A...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has been by far the worst service I have ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nov 2022 I booked and paid for a return jou...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA is not treating its premium economy passeng...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 hours before our departure on BA059 to Cape...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>LHR-JFK-LAX-LHR. Check in was ok apart from be...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>LHR to HAM. Purser addresses all club passenge...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>My son who had worked for British Airways urge...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>London City-New York JFK via Shannon on A318 b...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>SIN-LHR BA12 B747-436 First Class. Old aircraf...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3598 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Review Recommended\n",
       "0     My family and I have flown mostly on British A...          no\n",
       "1     This has been by far the worst service I have ...          no\n",
       "2     In Nov 2022 I booked and paid for a return jou...          no\n",
       "3     BA is not treating its premium economy passeng...          no\n",
       "4     24 hours before our departure on BA059 to Cape...          no\n",
       "...                                                 ...         ...\n",
       "3593  LHR-JFK-LAX-LHR. Check in was ok apart from be...          no\n",
       "3594  LHR to HAM. Purser addresses all club passenge...         yes\n",
       "3595  My son who had worked for British Airways urge...         yes\n",
       "3596  London City-New York JFK via Shannon on A318 b...          no\n",
       "3597  SIN-LHR BA12 B747-436 First Class. Old aircraf...          no\n",
       "\n",
       "[3598 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1469\n",
      "2129\n"
     ]
    }
   ],
   "source": [
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "for i in df3.index:\n",
    "    if df3.loc[i, \"Recommended\"] == 'yes':\n",
    "        positive_reviews.append(df3.loc[i, \"Review\"])\n",
    "    else:\n",
    "        negative_reviews.append(df3.loc[i, \"Review\"])\n",
    "print(len(positive_reviews))\n",
    "print(len(negative_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the train, test set\n",
    "test_pos = positive_reviews[1177:]\n",
    "train_pos = positive_reviews[:1177]\n",
    "test_neg = negative_reviews[1177:1469]\n",
    "train_neg = negative_reviews[:1177]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the numpy array of positive labels and negative labels.\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (2354, 1)\n",
      "test_y.shape = (584, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "* Punctuation Removal\n",
    "* Number removal\n",
    "* Lowering the Text\n",
    "* Tokenization i.e. split a review into words\n",
    "* Stop Word Removal: Stopwords are the commonly used words and are removed from the text as they do not add any value to the analysis\n",
    "* Stemming: Words are stemmed or diminished to their root/base form.  For example, ‘programmer’, ‘programming, ‘program’ are stemmed to ‘program’ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(review):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        :review: a string\n",
    "    Output:\n",
    "        :reviews_clean: a list of words containing the processed reviews\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    review = review.lower()\n",
    "    review = re.sub('\\[.*?\\]', '', review)\n",
    "    review = re.sub('[%s]' % re.escape(string.punctuation), '', review)\n",
    "    review = re.sub('\\w*\\d\\w*', '', review)\n",
    "    review = re.sub('[‘’“”…]', '', review)\n",
    "    review = re.sub('\\n', '', review)\n",
    "\n",
    "    # tokenize reviews\n",
    "    reviews_token = word_tokenize(review)\n",
    "\n",
    "    reviews_clean = []\n",
    "    for word in reviews_token:\n",
    "        if word not in stopwords_english:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            reviews_clean.append(stem_word)\n",
    "\n",
    "    return reviews_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flight fine. In-line with competitors. Pleasant crew. Only niggle is bussed to and from aircraft at Heathrow. It seems like this is standard for the Berlin flights. Would use again.\n",
      "['flight', 'fine', 'inlin', 'competitor', 'pleasant', 'crew', 'niggl', 'buss', 'aircraft', 'heathrow', 'seem', 'like', 'standard', 'berlin', 'flight', 'would', 'use']\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(process_review(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have used British Airways over a number of years for work and leisure trips non-exclusively and must say I have noted a gradual but continued decrease in service quality. On a recent return flight from Johannesburg travelling with an infant we were delayed for over 30 hours. The way in which the delay was handled was terrible. Minimal staff meant it took three plus hours to get checked in to a hotel. Then trying to arrange the return minimising waiting time for the little one took 5 hours with two staff serving 400 passengers with no concern for customers. Tried calling head office and they were less helpful than the booking agents.My intention is to avoid where possible and in many cases they are cheaper alternatives.\n",
      "['use', 'british', 'airway', 'number', 'year', 'work', 'leisur', 'trip', 'nonexclus', 'must', 'say', 'note', 'gradual', 'continu', 'decreas', 'servic', 'qualiti', 'recent', 'return', 'flight', 'johannesburg', 'travel', 'infant', 'delay', 'hour', 'way', 'delay', 'handl', 'terribl', 'minim', 'staff', 'meant', 'took', 'three', 'plu', 'hour', 'get', 'check', 'hotel', 'tri', 'arrang', 'return', 'minimis', 'wait', 'time', 'littl', 'one', 'took', 'hour', 'two', 'staff', 'serv', 'passeng', 'concern', 'custom', 'tri', 'call', 'head', 'offic', 'less', 'help', 'book', 'agentsmi', 'intent', 'avoid', 'possibl', 'mani', 'case', 'cheaper', 'altern']\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "print(process_review(test_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, machine models are made by mathematical formulae, hence, they can only understand numerical structures. Hence, we have to extract from our text data (reviews) specific information (<b>feature</b>) which are numerically interpretable and are linked to our goal (Sentiment Analysis). <br>\n",
    "In the present Sentiment Analysis problem, we want to build a machine which is capable of reading a review and tell us whether this review has a positive or negative sentiment. <br>\n",
    "   \n",
    "* We build a frequency dictionary {(word, label): freq} which count the number of times (frequency) that the word is associated with the label. \n",
    "    * For example, ('happy', 1): 14 means that the word \"happy\" appears in positive reviews 14 times.\n",
    "* Given a list of reviews, we will extract two features and store them into a matrix\n",
    "    * The first feature is the number of positive words in a review.\n",
    "    * The second feature is the number of negative words in a review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(reviews, ys):\n",
    "    \"\"\" Build frequencies\n",
    "    Input:\n",
    "    reviews: a list of reviews\n",
    "    ys: an mx1 array with the sentiment label of each review (either 0 or 1)\n",
    "    Output:\n",
    "    freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n",
    "    \"\"\"\n",
    "    yslist = np.squeeze(ys).tolist()##Remove axes of length one from a numpy darray\n",
    "    # start with an empty dict and populate it by looping over all tweets\n",
    "    freqs = {}\n",
    "    for y, review in zip(yslist, reviews):\n",
    "        for word in process_review(review):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11332\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('flight', 1.0): 2351,\n",
       " ('fine', 1.0): 138,\n",
       " ('inlin', 1.0): 1,\n",
       " ('competitor', 1.0): 17,\n",
       " ('pleasant', 1.0): 153,\n",
       " ('crew', 1.0): 916,\n",
       " ('niggl', 1.0): 4,\n",
       " ('buss', 1.0): 10,\n",
       " ('aircraft', 1.0): 314,\n",
       " ('heathrow', 1.0): 534,\n",
       " ('seem', 1.0): 181,\n",
       " ('like', 1.0): 240,\n",
       " ('standard', 1.0): 112,\n",
       " ('berlin', 1.0): 16,\n",
       " ('would', 1.0): 356,\n",
       " ('use', 1.0): 325,\n",
       " ('twoforon', 1.0): 1,\n",
       " ('review', 1.0): 82,\n",
       " ('cover', 1.0): 13,\n",
       " ('economi', 1.0): 459,\n",
       " ('busi', 1.0): 439,\n",
       " ('class', 1.0): 456,\n",
       " ('ba', 1.0): 1169,\n",
       " ('london', 1.0): 675,\n",
       " ('citi', 1.0): 41,\n",
       " ('ibiza', 1.0): 5,\n",
       " ('excel', 1.0): 351,\n",
       " ('light', 1.0): 53,\n",
       " ('lunch', 1.0): 53,\n",
       " ('small', 1.0): 158,\n",
       " ('cheeseandpastrami', 1.0): 1,\n",
       " ('sandwich', 1.0): 103,\n",
       " ('chocol', 1.0): 24,\n",
       " ('browni', 1.0): 2,\n",
       " ('follow', 1.0): 55,\n",
       " ('pretti', 1.0): 104,\n",
       " ('much', 1.0): 227,\n",
       " ('freeflow', 1.0): 1,\n",
       " ('drink', 1.0): 483,\n",
       " ('trolley', 1.0): 26,\n",
       " ('full', 1.0): 225,\n",
       " ('work', 1.0): 156,\n",
       " ('hard', 1.0): 56,\n",
       " ('good', 1.0): 1123,\n",
       " ('job', 1.0): 51,\n",
       " ('recommend', 1.0): 72,\n",
       " ('except', 1.0): 73,\n",
       " ('dont', 1.0): 106,\n",
       " ('fall', 1.0): 16,\n",
       " ('last', 1.0): 121,\n",
       " ('minut', 1.0): 218,\n",
       " ('offer', 1.0): 356,\n",
       " ('upgrad', 1.0): 98,\n",
       " ('£', 1.0): 71,\n",
       " ('direct', 1.0): 45,\n",
       " ('flew', 1.0): 207,\n",
       " ('back', 1.0): 254,\n",
       " ('brother', 1.0): 2,\n",
       " ('seat', 1.0): 1546,\n",
       " ('ident', 1.0): 7,\n",
       " ('block', 1.0): 16,\n",
       " ('squish', 1.0): 3,\n",
       " ('four', 1.0): 28,\n",
       " ('abreast', 1.0): 8,\n",
       " ('meal', 1.0): 346,\n",
       " ('almost', 1.0): 63,\n",
       " ('minor', 1.0): 12,\n",
       " ('tweak', 1.0): 1,\n",
       " ('yoghurt', 1.0): 8,\n",
       " ('real', 1.0): 37,\n",
       " ('plu', 1.0): 88,\n",
       " ('champagn', 1.0): 100,\n",
       " ('none', 1.0): 13,\n",
       " ('suppli', 1.0): 14,\n",
       " ('issu', 1.0): 93,\n",
       " ('cabin', 1.0): 767,\n",
       " ('attend', 1.0): 114,\n",
       " ('think', 1.0): 92,\n",
       " ('name', 1.0): 32,\n",
       " ('tom', 1.0): 2,\n",
       " ('must', 1.0): 29,\n",
       " ('one', 1.0): 392,\n",
       " ('best', 1.0): 173,\n",
       " ('didnt', 1.0): 121,\n",
       " ('eat', 1.0): 43,\n",
       " ('paid', 1.0): 70,\n",
       " ('noth', 1.0): 78,\n",
       " ('oh', 1.0): 3,\n",
       " ('well', 1.0): 346,\n",
       " ('great', 1.0): 324,\n",
       " ('stick', 1.0): 11,\n",
       " ('travel', 1.0): 273,\n",
       " ('kalamata', 1.0): 4,\n",
       " ('return', 1.0): 386,\n",
       " ('journey', 1.0): 94,\n",
       " ('day', 1.0): 162,\n",
       " ('later', 1.0): 49,\n",
       " ('world', 1.0): 257,\n",
       " ('aka', 1.0): 1,\n",
       " ('depart', 1.0): 90,\n",
       " ('time', 1.0): 854,\n",
       " ('land', 1.0): 211,\n",
       " ('earli', 1.0): 134,\n",
       " ('hour', 1.0): 343,\n",
       " ('wait', 1.0): 112,\n",
       " ('baggag', 1.0): 119,\n",
       " ('offload', 1.0): 4,\n",
       " ('rather', 1.0): 103,\n",
       " ('ruin', 1.0): 3,\n",
       " ('home', 1.0): 66,\n",
       " ('caught', 1.0): 5,\n",
       " ('motorway', 1.0): 1,\n",
       " ('closur', 1.0): 2,\n",
       " ('ok', 1.0): 161,\n",
       " ('fa', 1.0): 55,\n",
       " ('particularli', 1.0): 43,\n",
       " ('enough', 1.0): 127,\n",
       " ('usual', 1.0): 120,\n",
       " ('truli', 1.0): 14,\n",
       " ('dread', 1.0): 16,\n",
       " ('legroom', 1.0): 120,\n",
       " ('better', 1.0): 228,\n",
       " ('club', 1.0): 423,\n",
       " ('either', 1.0): 43,\n",
       " ('middl', 1.0): 85,\n",
       " ('left', 1.0): 108,\n",
       " ('free', 1.0): 104,\n",
       " ('mark', 1.0): 17,\n",
       " ('tall', 1.0): 23,\n",
       " ('british', 1.0): 584,\n",
       " ('airway', 1.0): 585,\n",
       " ('welcom', 1.0): 94,\n",
       " ('aboard', 1.0): 7,\n",
       " ('warm', 1.0): 36,\n",
       " ('continu', 1.0): 40,\n",
       " ('throughout', 1.0): 64,\n",
       " ('attent', 1.0): 151,\n",
       " ('friendli', 1.0): 365,\n",
       " ('profession', 1.0): 180,\n",
       " ('board', 1.0): 627,\n",
       " ('food', 1.0): 808,\n",
       " ('dinner', 1.0): 99,\n",
       " ('breakfast', 1.0): 215,\n",
       " ('chosen', 1.0): 10,\n",
       " ('select', 1.0): 194,\n",
       " ('wine', 1.0): 140,\n",
       " ('entertain', 1.0): 205,\n",
       " ('film', 1.0): 60,\n",
       " ('audio', 1.0): 10,\n",
       " ('seatflat', 1.0): 1,\n",
       " ('bed', 1.0): 74,\n",
       " ('comfort', 1.0): 460,\n",
       " ('done', 1.0): 64,\n",
       " ('design', 1.0): 22,\n",
       " ('suit', 1.0): 39,\n",
       " ('sleek', 1.0): 2,\n",
       " ('minimalist', 1.0): 3,\n",
       " ('show', 1.0): 79,\n",
       " ('among', 1.0): 8,\n",
       " ('airlin', 1.0): 312,\n",
       " ('seriou', 1.0): 6,\n",
       " ('medic', 1.0): 7,\n",
       " ('problem', 1.0): 109,\n",
       " ('appear', 1.0): 43,\n",
       " ('onboard', 1.0): 82,\n",
       " ('inform', 1.0): 91,\n",
       " ('help', 1.0): 224,\n",
       " ('unfortun', 1.0): 41,\n",
       " ('transit', 1.0): 15,\n",
       " ('becam', 1.0): 14,\n",
       " ('difficult', 1.0): 33,\n",
       " ('unwel', 1.0): 2,\n",
       " ('also', 1.0): 207,\n",
       " ('could', 1.0): 203,\n",
       " ('find', 1.0): 84,\n",
       " ('way', 1.0): 199,\n",
       " ('condit', 1.0): 20,\n",
       " ('thank', 1.0): 84,\n",
       " ('everi', 1.0): 56,\n",
       " ('step', 1.0): 26,\n",
       " ('stress', 1.0): 20,\n",
       " ('experi', 1.0): 324,\n",
       " ('made', 1.0): 202,\n",
       " ('toler', 1.0): 3,\n",
       " ('lhr', 1.0): 172,\n",
       " ('loung', 1.0): 452,\n",
       " ('crowd', 1.0): 33,\n",
       " ('b', 1.0): 15,\n",
       " ('gate', 1.0): 163,\n",
       " ('quieter', 1.0): 10,\n",
       " ('greet', 1.0): 42,\n",
       " ('execut', 1.0): 18,\n",
       " ('statu', 1.0): 26,\n",
       " ('acknowledg', 1.0): 4,\n",
       " ('custom', 1.0): 89,\n",
       " ('nice', 1.0): 294,\n",
       " ('touch', 1.0): 36,\n",
       " ('servic', 1.0): 903,\n",
       " ('averag', 1.0): 56,\n",
       " ('arriv', 1.0): 354,\n",
       " ('quick', 1.0): 145,\n",
       " ('process', 1.0): 59,\n",
       " ('jfk', 1.0): 45,\n",
       " ('easi', 1.0): 71,\n",
       " ('check', 1.0): 350,\n",
       " ('valencia', 1.0): 4,\n",
       " ('book', 1.0): 176,\n",
       " ('afternoon', 1.0): 45,\n",
       " ('long', 1.0): 224,\n",
       " ('haul', 1.0): 122,\n",
       " ('anoth', 1.0): 96,\n",
       " ('connect', 1.0): 102,\n",
       " ('ask', 1.0): 118,\n",
       " ('put', 1.0): 65,\n",
       " ('onto', 1.0): 29,\n",
       " ('morn', 1.0): 50,\n",
       " ('avoid', 1.0): 30,\n",
       " ('seven', 1.0): 5,\n",
       " ('chang', 1.0): 88,\n",
       " ('cost', 1.0): 77,\n",
       " ('within', 1.0): 38,\n",
       " ('five', 1.0): 9,\n",
       " ('inde', 1.0): 20,\n",
       " ('onlin', 1.0): 70,\n",
       " ('secur', 1.0): 152,\n",
       " ('glasgow', 1.0): 33,\n",
       " ('water', 1.0): 77,\n",
       " ('snack', 1.0): 166,\n",
       " ('provid', 1.0): 123,\n",
       " ('pleas', 1.0): 26,\n",
       " ('cheap', 1.0): 23,\n",
       " ('effici', 1.0): 217,\n",
       " ('second', 1.0): 46,\n",
       " ('premium', 1.0): 182,\n",
       " ('newer', 1.0): 21,\n",
       " ('yet', 1.0): 22,\n",
       " ('despit', 1.0): 81,\n",
       " ('three', 1.0): 47,\n",
       " ('delay', 1.0): 240,\n",
       " ('miss', 1.0): 68,\n",
       " ('hong', 1.0): 27,\n",
       " ('kong', 1.0): 27,\n",
       " ('spaciou', 1.0): 48,\n",
       " ('layout', 1.0): 42,\n",
       " ('attitud', 1.0): 22,\n",
       " ('commun', 1.0): 13,\n",
       " ('rebook', 1.0): 24,\n",
       " ('brought', 1.0): 17,\n",
       " ('peac', 1.0): 1,\n",
       " ('mind', 1.0): 15,\n",
       " ('major', 1.0): 27,\n",
       " ('disappoint', 1.0): 89,\n",
       " ('earlier', 1.0): 26,\n",
       " ('year', 1.0): 157,\n",
       " ('enjoy', 1.0): 112,\n",
       " ('swift', 1.0): 10,\n",
       " ('qualiti', 1.0): 147,\n",
       " ('downsid', 1.0): 19,\n",
       " ('ground', 1.0): 95,\n",
       " ('readi', 1.0): 16,\n",
       " ('total', 1.0): 39,\n",
       " ('get', 1.0): 313,\n",
       " ('door', 1.0): 29,\n",
       " ('open', 1.0): 46,\n",
       " ('took', 1.0): 139,\n",
       " ('believ', 1.0): 22,\n",
       " ('strike', 1.0): 18,\n",
       " ('action', 1.0): 10,\n",
       " ('week', 1.0): 38,\n",
       " ('swore', 1.0): 1,\n",
       " ('never', 1.0): 57,\n",
       " ('fli', 1.0): 349,\n",
       " ('mess', 1.0): 11,\n",
       " ('tri', 1.0): 113,\n",
       " ('budapest', 1.0): 8,\n",
       " ('decid', 1.0): 35,\n",
       " ('give', 1.0): 73,\n",
       " ('go', 1.0): 136,\n",
       " ('clean', 1.0): 188,\n",
       " ('staff', 1.0): 404,\n",
       " ('even', 1.0): 216,\n",
       " ('complimentari', 1.0): 33,\n",
       " ('teacoffe', 1.0): 4,\n",
       " ('bar', 1.0): 67,\n",
       " ('cant', 1.0): 41,\n",
       " ('neg', 1.0): 44,\n",
       " ('write', 1.0): 17,\n",
       " ('certainli', 1.0): 28,\n",
       " ('head', 1.0): 18,\n",
       " ('shoulder', 1.0): 8,\n",
       " ('trip', 1.0): 158,\n",
       " ('easyjet', 1.0): 27,\n",
       " ('mumbai', 1.0): 14,\n",
       " ('chaotic', 1.0): 23,\n",
       " ('badli', 1.0): 6,\n",
       " ('organis', 1.0): 34,\n",
       " ('announc', 1.0): 42,\n",
       " ('relax', 1.0): 42,\n",
       " ('typic', 1.0): 19,\n",
       " ('style', 1.0): 35,\n",
       " ('quit', 1.0): 205,\n",
       " ('refurbish', 1.0): 32,\n",
       " ('pad', 1.0): 10,\n",
       " ('materi', 1.0): 6,\n",
       " ('lucki', 1.0): 17,\n",
       " ('empti', 1.0): 52,\n",
       " ('suffer', 1.0): 9,\n",
       " ('new', 1.0): 310,\n",
       " ('narrow', 1.0): 54,\n",
       " ('lot', 1.0): 93,\n",
       " ('less', 1.0): 77,\n",
       " ('ife', 1.0): 164,\n",
       " ('respons', 1.0): 21,\n",
       " ('enorm', 1.0): 3,\n",
       " ('tv', 1.0): 49,\n",
       " ('movi', 1.0): 60,\n",
       " ('number', 1.0): 39,\n",
       " ('game', 1.0): 17,\n",
       " ('wifi', 1.0): 34,\n",
       " ('netflix', 1.0): 2,\n",
       " ('still', 1.0): 177,\n",
       " ('reason', 1.0): 140,\n",
       " ('bad', 1.0): 75,\n",
       " ('valu', 1.0): 59,\n",
       " ('pay', 1.0): 119,\n",
       " ('want', 1.0): 91,\n",
       " ('short', 1.0): 170,\n",
       " ('term', 1.0): 20,\n",
       " ('simpl', 1.0): 15,\n",
       " ('messag', 1.0): 10,\n",
       " ('cater', 1.0): 45,\n",
       " ('although', 1.0): 198,\n",
       " ('near', 1.0): 30,\n",
       " ('tasti', 1.0): 71,\n",
       " ('pastri', 1.0): 6,\n",
       " ('wasnt', 1.0): 70,\n",
       " ('realli', 1.0): 278,\n",
       " ('astonishingli', 1.0): 2,\n",
       " ('felt', 1.0): 72,\n",
       " ('though', 1.0): 131,\n",
       " ('queue', 1.0): 102,\n",
       " ('super', 1.0): 17,\n",
       " ('come', 1.0): 91,\n",
       " ('overal', 1.0): 213,\n",
       " ('solid', 1.0): 11,\n",
       " ('significantli', 1.0): 9,\n",
       " ('verifi', 1.0): 4,\n",
       " ('mexico', 1.0): 12,\n",
       " ('airport', 1.0): 158,\n",
       " ('zoo', 1.0): 1,\n",
       " ('take', 1.0): 170,\n",
       " ('late', 1.0): 152,\n",
       " ('departur', 1.0): 146,\n",
       " ('isnt', 1.0): 23,\n",
       " ('passeng', 1.0): 309,\n",
       " ('aa', 1.0): 16,\n",
       " ('admir', 1.0): 3,\n",
       " ('surprisingli', 1.0): 27,\n",
       " ('love', 1.0): 73,\n",
       " ('older', 1.0): 51,\n",
       " ('old', 1.0): 197,\n",
       " ('front', 1.0): 101,\n",
       " ('row', 1.0): 130,\n",
       " ('window', 1.0): 99,\n",
       " ('adjec', 1.0): 1,\n",
       " ('aisl', 1.0): 69,\n",
       " ('partner', 1.0): 15,\n",
       " ('cut', 1.0): 37,\n",
       " ('edg', 1.0): 8,\n",
       " ('coupl', 1.0): 62,\n",
       " ('unobstruct', 1.0): 1,\n",
       " ('access', 1.0): 53,\n",
       " ('terribl', 1.0): 17,\n",
       " ('obvioulsi', 1.0): 1,\n",
       " ('happi', 1.0): 78,\n",
       " ('amaz', 1.0): 55,\n",
       " ('impec', 1.0): 1,\n",
       " ('right', 1.0): 41,\n",
       " ('mix', 1.0): 32,\n",
       " ('humor', 1.0): 3,\n",
       " ('hardli', 1.0): 12,\n",
       " ('sleep', 1.0): 145,\n",
       " ('smooth', 1.0): 86,\n",
       " ('went', 1.0): 98,\n",
       " ('outsid', 1.0): 9,\n",
       " ('smoke', 1.0): 7,\n",
       " ('termin', 1.0): 143,\n",
       " ('fast', 1.0): 111,\n",
       " ('track', 1.0): 54,\n",
       " ('alway', 1.0): 111,\n",
       " ('today', 1.0): 7,\n",
       " ('chat', 1.0): 20,\n",
       " ('bag', 1.0): 231,\n",
       " ('without', 1.0): 75,\n",
       " ('secundari', 1.0): 1,\n",
       " ('shower', 1.0): 18,\n",
       " ('spa', 1.0): 9,\n",
       " ('fabul', 1.0): 8,\n",
       " ('facil', 1.0): 15,\n",
       " ('south', 1.0): 41,\n",
       " ('crazi', 1.0): 4,\n",
       " ('disast', 1.0): 4,\n",
       " ('ever', 1.0): 40,\n",
       " ('choos', 1.0): 60,\n",
       " ('bizarr', 1.0): 8,\n",
       " ('combin', 1.0): 7,\n",
       " ('furnitur', 1.0): 3,\n",
       " ('stretch', 1.0): 25,\n",
       " ('amsterdam', 1.0): 20,\n",
       " ('straight', 1.0): 27,\n",
       " ('forward', 1.0): 51,\n",
       " ('lousi', 1.0): 2,\n",
       " ('recov', 1.0): 3,\n",
       " ('everyon', 1.0): 40,\n",
       " ('feel', 1.0): 115,\n",
       " ('special', 1.0): 41,\n",
       " ('barcart', 1.0): 1,\n",
       " ('doubleserv', 1.0): 1,\n",
       " ('english', 1.0): 30,\n",
       " ('tea', 1.0): 98,\n",
       " ('worst', 1.0): 16,\n",
       " ('concept', 1.0): 4,\n",
       " ('understand', 1.0): 36,\n",
       " ('stuck', 1.0): 13,\n",
       " ('noon', 1.0): 3,\n",
       " ('cucumb', 1.0): 3,\n",
       " ('white', 1.0): 19,\n",
       " ('bread', 1.0): 22,\n",
       " ('thing', 1.0): 96,\n",
       " ('hey', 1.0): 3,\n",
       " ('taxi', 1.0): 13,\n",
       " ('clumsi', 1.0): 1,\n",
       " ('jet', 1.0): 13,\n",
       " ('bridg', 1.0): 10,\n",
       " ('end', 1.0): 64,\n",
       " ('jetbridg', 1.0): 1,\n",
       " ('wonder', 1.0): 49,\n",
       " ('flown', 1.0): 77,\n",
       " ('repeatedli', 1.0): 3,\n",
       " ('allianc', 1.0): 2,\n",
       " ('miami', 1.0): 25,\n",
       " ('massiv', 1.0): 11,\n",
       " ('final', 1.0): 38,\n",
       " ('luggag', 1.0): 107,\n",
       " ('puerto', 1.0): 2,\n",
       " ('rico', 1.0): 2,\n",
       " ('reschedul', 1.0): 4,\n",
       " ('round', 1.0): 35,\n",
       " ('havent', 1.0): 13,\n",
       " ('receiv', 1.0): 36,\n",
       " ('luckili', 1.0): 7,\n",
       " ('meant', 1.0): 47,\n",
       " ('plane', 1.0): 307,\n",
       " ('anyth', 1.0): 41,\n",
       " ('wir', 1.0): 7,\n",
       " ('sind', 1.0): 1,\n",
       " ('den', 1.0): 1,\n",
       " ('letzten', 1.0): 2,\n",
       " ('wochen', 1.0): 1,\n",
       " ('immer', 1.0): 1,\n",
       " ('wieder', 1.0): 3,\n",
       " ('mit', 1.0): 2,\n",
       " ('britisch', 1.0): 1,\n",
       " ('und', 1.0): 6,\n",
       " ('dem', 1.0): 1,\n",
       " ('zusammenschluss', 1.0): 1,\n",
       " ('geflogen', 1.0): 1,\n",
       " ('jeder', 1.0): 1,\n",
       " ('flug', 1.0): 2,\n",
       " ('war', 1.0): 4,\n",
       " ('verspätet', 1.0): 1,\n",
       " ('nun', 1.0): 1,\n",
       " ('sitzen', 1.0): 2,\n",
       " ('flughafen', 1.0): 2,\n",
       " ('haben', 1.0): 3,\n",
       " ('schon', 1.0): 1,\n",
       " ('ein', 1.0): 1,\n",
       " ('verspätung', 1.0): 2,\n",
       " ('sodass', 1.0): 1,\n",
       " ('unseren', 1.0): 2,\n",
       " ('anschlussflug', 1.0): 1,\n",
       " ('verpassen', 1.0): 2,\n",
       " ('werden', 1.0): 1,\n",
       " ('zuletzt', 1.0): 1,\n",
       " ('ist', 1.0): 1,\n",
       " ('auch', 1.0): 2,\n",
       " ('unser', 1.0): 2,\n",
       " ('gepäck', 1.0): 2,\n",
       " ('tagen', 1.0): 1,\n",
       " ('angekommen', 1.0): 1,\n",
       " ('mussten', 1.0): 1,\n",
       " ('rundreis', 1.0): 1,\n",
       " ('umplanen', 1.0): 1,\n",
       " ('bi', 1.0): 1,\n",
       " ('heut', 1.0): 1,\n",
       " ('noch', 1.0): 1,\n",
       " ('kein', 1.0): 1,\n",
       " ('info', 1.0): 4,\n",
       " ('über', 1.0): 1,\n",
       " ('erhalten', 1.0): 1,\n",
       " ('zum', 1.0): 1,\n",
       " ('glück', 1.0): 1,\n",
       " ('selbst', 1.0): 1,\n",
       " ('nachgesehen', 1.0): 1,\n",
       " ('es', 1.0): 1,\n",
       " ('da', 1.0): 1,\n",
       " ('beim', 1.0): 1,\n",
       " ('verpspäteten', 1.0): 1,\n",
       " ('meint', 1.0): 1,\n",
       " ('die', 1.0): 5,\n",
       " ('kabinen', 1.0): 1,\n",
       " ('dass', 1.0): 1,\n",
       " ('sie', 1.0): 2,\n",
       " ('ihr', 1.0): 1,\n",
       " ('anschlussflüg', 1.0): 1,\n",
       " ('im', 1.0): 58,\n",
       " ('selben', 1.0): 1,\n",
       " ('flugzeug', 1.0): 1,\n",
       " ('können', 1.0): 1,\n",
       " ('nicht', 1.0): 1,\n",
       " ('machen', 1.0): 1,\n",
       " ('first', 1.0): 422,\n",
       " ('america', 1.0): 6,\n",
       " ('india', 1.0): 8,\n",
       " ('via', 1.0): 135,\n",
       " ('chose', 1.0): 27,\n",
       " ('price', 1.0): 101,\n",
       " ('howev', 1.0): 183,\n",
       " ('leg', 1.0): 234,\n",
       " ('bit', 1.0): 173,\n",
       " ('outdat', 1.0): 19,\n",
       " ('foot', 1.0): 14,\n",
       " ('rest', 1.0): 55,\n",
       " ('nassau', 1.0): 5,\n",
       " ('brand', 1.0): 10,\n",
       " ('pop', 1.0): 2,\n",
       " ('support', 1.0): 12,\n",
       " ('appreci', 1.0): 47,\n",
       " ('often', 1.0): 24,\n",
       " ('werent', 1.0): 16,\n",
       " ('honest', 1.0): 11,\n",
       " ('tarmac', 1.0): 9,\n",
       " ('move', 1.0): 53,\n",
       " ('quickli', 1.0): 77,\n",
       " ('spare', 1.0): 7,\n",
       " ('next', 1.0): 94,\n",
       " ('found', 1.0): 102,\n",
       " ('make', 1.0): 146,\n",
       " ('due', 1.0): 160,\n",
       " ('repres', 1.0): 9,\n",
       " ('extrem', 1.0): 48,\n",
       " ('claim', 1.0): 10,\n",
       " ('deliv', 1.0): 41,\n",
       " ('charter', 1.0): 3,\n",
       " ('ship', 1.0): 2,\n",
       " ('island', 1.0): 2,\n",
       " ('stay', 1.0): 17,\n",
       " ('impress', 1.0): 77,\n",
       " ('dubrovnik', 1.0): 9,\n",
       " ('pain', 1.0): 7,\n",
       " ('gatwick', 1.0): 199,\n",
       " ('especi', 1.0): 74,\n",
       " ('minimum', 1.0): 9,\n",
       " ('ryanair', 1.0): 26,\n",
       " ('wizzair', 1.0): 1,\n",
       " ('odd', 1.0): 16,\n",
       " ('utilis', 1.0): 3,\n",
       " ('cheaper', 1.0): 19,\n",
       " ('slot', 1.0): 9,\n",
       " ('choic', 1.0): 212,\n",
       " ('given', 1.0): 126,\n",
       " ('peopl', 1.0): 108,\n",
       " ('us', 1.0): 221,\n",
       " ('base', 1.0): 17,\n",
       " ('alon', 1.0): 12,\n",
       " ('outbound', 1.0): 97,\n",
       " ('lgwdbv', 1.0): 2,\n",
       " ('fairli', 1.0): 44,\n",
       " ('pilot', 1.0): 34,\n",
       " ('clear', 1.0): 42,\n",
       " ('inspir', 1.0): 5,\n",
       " ('confid', 1.0): 5,\n",
       " ('someth', 1.0): 51,\n",
       " ('expect', 1.0): 117,\n",
       " ('recent', 1.0): 59,\n",
       " ('program', 1.0): 11,\n",
       " ('advantag', 1.0): 12,\n",
       " ('pinnacl', 1.0): 2,\n",
       " ('adjust', 1.0): 11,\n",
       " ('wing', 1.0): 8,\n",
       " ('headrest', 1.0): 16,\n",
       " ('harrog', 1.0): 2,\n",
       " ('miner', 1.0): 1,\n",
       " ('nutrigain', 1.0): 1,\n",
       " ('poor', 1.0): 104,\n",
       " ('unpolish', 1.0): 2,\n",
       " ('present', 1.0): 44,\n",
       " ('behaviour', 1.0): 2,\n",
       " ('messi', 1.0): 4,\n",
       " ('illfit', 1.0): 1,\n",
       " ('uniform', 1.0): 8,\n",
       " ('unclassi', 1.0): 1,\n",
       " ('makeup', 1.0): 3,\n",
       " ('toilet', 1.0): 78,\n",
       " ('maintain', 1.0): 22,\n",
       " ('antibacteri', 1.0): 1,\n",
       " ('wipe', 1.0): 5,\n",
       " ('kept', 1.0): 82,\n",
       " ('avail', 1.0): 118,\n",
       " ('postcovid', 1.0): 3,\n",
       " ('inbound', 1.0): 27,\n",
       " ('dbvlgw', 1.0): 2,\n",
       " ('app', 1.0): 14,\n",
       " ('checkin', 1.0): 138,\n",
       " ('modern', 1.0): 24,\n",
       " ('state', 1.0): 16,\n",
       " ('art', 1.0): 3,\n",
       " ('manual', 1.0): 2,\n",
       " ('proactiv', 1.0): 9,\n",
       " ('larger', 1.0): 16,\n",
       " ('handluggag', 1.0): 2,\n",
       " ('hold', 1.0): 14,\n",
       " ('charg', 1.0): 63,\n",
       " ('keep', 1.0): 43,\n",
       " ('dash', 1.0): 1,\n",
       " ('train', 1.0): 24,\n",
       " ('cancel', 1.0): 41,\n",
       " ('alreadi', 1.0): 27,\n",
       " ('season', 1.0): 9,\n",
       " ('sole', 1.0): 5,\n",
       " ('includ', 1.0): 85,\n",
       " ('sat', 1.0): 38,\n",
       " ('manag', 1.0): 105,\n",
       " ('oper', 1.0): 30,\n",
       " ('live', 1.0): 9,\n",
       " ('purpos', 1.0): 7,\n",
       " ('enabl', 1.0): 8,\n",
       " ('unclear', 1.0): 3,\n",
       " ('couldnt', 1.0): 58,\n",
       " ('rememb', 1.0): 8,\n",
       " ('apolog', 1.0): 11,\n",
       " ('smarter', 1.0): 2,\n",
       " ('approach', 1.0): 19,\n",
       " ('tiniest', 1.0): 1,\n",
       " ('pretzel', 1.0): 7,\n",
       " ('horrend', 1.0): 5,\n",
       " ('slimlin', 1.0): 4,\n",
       " ('instal', 1.0): 6,\n",
       " ('onward', 1.0): 5,\n",
       " ('bare', 1.0): 13,\n",
       " ('catch', 1.0): 13,\n",
       " ('uncancel', 1.0): 1,\n",
       " ('otherwis', 1.0): 27,\n",
       " ('wed', 1.0): 4,\n",
       " ('strand', 1.0): 3,\n",
       " ('spent', 1.0): 17,\n",
       " ('half', 1.0): 57,\n",
       " ('circl', 1.0): 2,\n",
       " ('origin', 1.0): 22,\n",
       " ('context', 1.0): 3,\n",
       " ('shorthaul', 1.0): 20,\n",
       " ('market', 1.0): 1,\n",
       " ('that', 1.0): 25,\n",
       " ('deserv', 1.0): 10,\n",
       " ('lowcost', 1.0): 18,\n",
       " ('carrier', 1.0): 75,\n",
       " ('mani', 1.0): 99,\n",
       " ('measur', 1.0): 4,\n",
       " ('least', 1.0): 47,\n",
       " ('option', 1.0): 124,\n",
       " ('point', 1.0): 54,\n",
       " ('extra', 1.0): 120,\n",
       " ('euro', 1.0): 17,\n",
       " ('product', 1.0): 116,\n",
       " ('piti', 1.0): 3,\n",
       " ('sale', 1.0): 16,\n",
       " ('excit', 1.0): 13,\n",
       " ('appeal', 1.0): 5,\n",
       " ('non', 1.0): 11,\n",
       " ('exist', 1.0): 9,\n",
       " ('hot', 1.0): 101,\n",
       " ('self', 1.0): 11,\n",
       " ('know', 1.0): 61,\n",
       " ('plenti', 1.0): 107,\n",
       " ('assist', 1.0): 33,\n",
       " ('hand', 1.0): 69,\n",
       " ('happili', 1.0): 10,\n",
       " ('person', 1.0): 88,\n",
       " ('emerg', 1.0): 18,\n",
       " ('allow', 1.0): 64,\n",
       " ('lhrist', 1.0): 2,\n",
       " ('frill', 1.0): 3,\n",
       " ('jfklhr', 1.0): 1,\n",
       " ('impecc', 1.0): 7,\n",
       " ('moviescd', 1.0): 1,\n",
       " ('entir', 1.0): 25,\n",
       " ('drinksnack', 1.0): 1,\n",
       " ('uncomfort', 1.0): 31,\n",
       " ('possibl', 1.0): 33,\n",
       " ('look', 1.0): 173,\n",
       " ('decent', 1.0): 93,\n",
       " ('strang', 1.0): 10,\n",
       " ('configur', 1.0): 43,\n",
       " ('neighbour', 1.0): 12,\n",
       " ('sit', 1.0): 67,\n",
       " ('opposit', 1.0): 8,\n",
       " ('posit', 1.0): 66,\n",
       " ('nearli', 1.0): 20,\n",
       " ('amen', 1.0): 28,\n",
       " ('aperit', 1.0): 1,\n",
       " ('pantri', 1.0): 1,\n",
       " ('asid', 1.0): 10,\n",
       " ('european', 1.0): 32,\n",
       " ('slow', 1.0): 61,\n",
       " ('spontan', 1.0): 1,\n",
       " ('refil', 1.0): 3,\n",
       " ('accept', 1.0): 57,\n",
       " ('continent', 1.0): 7,\n",
       " ('restroom', 1.0): 4,\n",
       " ('lotion', 1.0): 2,\n",
       " ('similar', 1.0): 25,\n",
       " ('label', 1.0): 8,\n",
       " ('prioriti', 1.0): 96,\n",
       " ('caesar', 1.0): 3,\n",
       " ('salad', 1.0): 38,\n",
       " ('mouss', 1.0): 10,\n",
       " ('sweet', 1.0): 11,\n",
       " ('wrong', 1.0): 18,\n",
       " ('pleasantli', 1.0): 32,\n",
       " ('surpris', 1.0): 87,\n",
       " ('islamabad', 1.0): 2,\n",
       " ('doha', 1.0): 12,\n",
       " ('qatar', 1.0): 19,\n",
       " ('gener', 1.0): 67,\n",
       " ('stop', 1.0): 27,\n",
       " ('hassl', 1.0): 14,\n",
       " ('let', 1.0): 55,\n",
       " ('content', 1.0): 9,\n",
       " ('huge', 1.0): 38,\n",
       " ('incom', 1.0): 6,\n",
       " ('england', 1.0): 6,\n",
       " ('pmilgw', 1.0): 1,\n",
       " ('wizz', 1.0): 1,\n",
       " ('air', 1.0): 83,\n",
       " ('inconveni', 1.0): 8,\n",
       " ('worth', 1.0): 64,\n",
       " ('conveni', 1.0): 9,\n",
       " ('ideal', 1.0): 6,\n",
       " ('€', 1.0): 4,\n",
       " ('per', 1.0): 22,\n",
       " ('regular', 1.0): 45,\n",
       " ('updat', 1.0): 35,\n",
       " ('email', 1.0): 12,\n",
       " ('seamless', 1.0): 8,\n",
       " ('alloc', 1.0): 17,\n",
       " ('desk', 1.0): 37,\n",
       " ('pmi', 1.0): 1,\n",
       " ('lgw', 1.0): 31,\n",
       " ('ensur', 1.0): 17,\n",
       " ('dealt', 1.0): 10,\n",
       " ('humanli', 1.0): 1,\n",
       " ('immacul', 1.0): 5,\n",
       " ('leather', 1.0): 18,\n",
       " ('adequ', 1.0): 44,\n",
       " ('polit', 1.0): 71,\n",
       " ('meat', 1.0): 9,\n",
       " ('ploughman', 1.0): 1,\n",
       " ('ipa', 1.0): 2,\n",
       " ('beer', 1.0): 10,\n",
       " ('differ', 1.0): 108,\n",
       " ('variou', 1.0): 4,\n",
       " ('propel', 1.0): 1,\n",
       " ('lgwlhr', 1.0): 1,\n",
       " ('helicopt', 1.0): 1,\n",
       " ('transfer', 1.0): 34,\n",
       " ('age', 1.0): 69,\n",
       " ('budget', 1.0): 16,\n",
       " ('nostalgia', 1.0): 1,\n",
       " ('ive', 1.0): 73,\n",
       " ('husband', 1.0): 15,\n",
       " ('madrid', 1.0): 21,\n",
       " ('februari', 1.0): 6,\n",
       " ('legal', 1.0): 2,\n",
       " ('matter', 1.0): 18,\n",
       " ('pm', 1.0): 9,\n",
       " ('car', 1.0): 11,\n",
       " ('holiday', 1.0): 45,\n",
       " ('park', 1.0): 19,\n",
       " ('got', 1.0): 154,\n",
       " ('lost', 1.0): 24,\n",
       " ('calm', 1.0): 11,\n",
       " ('cold', 1.0): 31,\n",
       " ('ladi', 1.0): 30,\n",
       " ('spoke', 1.0): 10,\n",
       " ('gave', 1.0): 58,\n",
       " ('call', 1.0): 75,\n",
       " ('told', 1.0): 48,\n",
       " ('els', 1.0): 36,\n",
       " ('till', 1.0): 7,\n",
       " ('explain', 1.0): 26,\n",
       " ('reach', 1.0): 18,\n",
       " ('young', 1.0): 33,\n",
       " ('althea', 1.0): 2,\n",
       " ('start', 1.0): 80,\n",
       " ('sever', 1.0): 44,\n",
       " ('someon', 1.0): 27,\n",
       " ('offic', 1.0): 13,\n",
       " ('send', 1.0): 2,\n",
       " ('text', 1.0): 8,\n",
       " ('answer', 1.0): 10,\n",
       " ('line', 1.0): 45,\n",
       " ('modifi', 1.0): 2,\n",
       " ('place', 1.0): 40,\n",
       " ('reserv', 1.0): 20,\n",
       " ('care', 1.0): 49,\n",
       " ('determin', 1.0): 1,\n",
       " ('sure', 1.0): 50,\n",
       " ('beyond', 1.0): 15,\n",
       " ('role', 1.0): 2,\n",
       " ('credit', 1.0): 23,\n",
       " ('europ', 1.0): 148,\n",
       " ('might', 1.0): 29,\n",
       " ('lead', 1.0): 8,\n",
       " ('around', 1.0): 96,\n",
       " ('ticket', 1.0): 54,\n",
       " ('concord', 1.0): 33,\n",
       " ('room', 1.0): 153,\n",
       " ('prior', 1.0): 35,\n",
       " ('slightli', 1.0): 73,\n",
       " ('schedul', 1.0): 83,\n",
       " ('johannesburg', 1.0): 34,\n",
       " ('cape', 1.0): 34,\n",
       " ('town', 1.0): 36,\n",
       " ('high', 1.0): 47,\n",
       " ('sound', 1.0): 14,\n",
       " ('overcrowd', 1.0): 8,\n",
       " ('edibl', 1.0): 15,\n",
       " ('compar', 1.0): 81,\n",
       " ('ago', 1.0): 22,\n",
       " ('previou', 1.0): 48,\n",
       " ('individu', 1.0): 19,\n",
       " ('bland', 1.0): 8,\n",
       " ('blend', 1.0): 1,\n",
       " ('cours', 1.0): 62,\n",
       " ('serv', 1.0): 315,\n",
       " ('glass', 1.0): 50,\n",
       " ('china', 1.0): 4,\n",
       " ('beef', 1.0): 44,\n",
       " ('cheek', 1.0): 8,\n",
       " ('soup', 1.0): 9,\n",
       " ('night', 1.0): 47,\n",
       " ('fruit', 1.0): 16,\n",
       " ('etc', 1.0): 43,\n",
       " ('gulf', 1.0): 1,\n",
       " ('eventu', 1.0): 18,\n",
       " ('quiet', 1.0): 62,\n",
       " ('tel', 1.0): 10,\n",
       " ('aviv', 1.0): 10,\n",
       " ('easili', 1.0): 8,\n",
       " ('space', 1.0): 125,\n",
       " ('requir', 1.0): 27,\n",
       " ('upon', 1.0): 26,\n",
       " ('break', 1.0): 13,\n",
       " ('overh', 1.0): 2,\n",
       " ('fire', 1.0): 4,\n",
       " ('cool', 1.0): 3,\n",
       " ('part', 1.0): 46,\n",
       " ('actual', 1.0): 53,\n",
       " ('taken', 1.0): 34,\n",
       " ('correct', 1.0): 1,\n",
       " ('member', 1.0): 79,\n",
       " ('specif', 1.0): 10,\n",
       " ('loyalti', 1.0): 6,\n",
       " ('galleri', 1.0): 50,\n",
       " ('north', 1.0): 26,\n",
       " ('wife', 1.0): 62,\n",
       " ('damag', 1.0): 10,\n",
       " ('storag', 1.0): 47,\n",
       " ('area', 1.0): 57,\n",
       " ('run', 1.0): 50,\n",
       " ('slept', 1.0): 25,\n",
       " ('harder', 1.0): 9,\n",
       " ('shuttl', 1.0): 11,\n",
       " ('across', 1.0): 22,\n",
       " ('uk', 1.0): 29,\n",
       " ('doesnt', 1.0): 14,\n",
       " ('comment', 1.0): 20,\n",
       " ('fresh', 1.0): 23,\n",
       " ('distanc', 1.0): 14,\n",
       " ('walk', 1.0): 38,\n",
       " ('covid', 1.0): 17,\n",
       " ('bottl', 1.0): 33,\n",
       " ('cup', 1.0): 30,\n",
       " ('add', 1.0): 16,\n",
       " ('current', 1.0): 15,\n",
       " ('sky', 1.0): 11,\n",
       " ('cafe', 1.0): 4,\n",
       " ('fiddli', 1.0): 1,\n",
       " ('pitch', 1.0): 24,\n",
       " ('perfectli', 1.0): 20,\n",
       " ('constant', 1.0): 7,\n",
       " ('cockpit', 1.0): 15,\n",
       " ('chip', 1.0): 6,\n",
       " ('deliveri', 1.0): 8,\n",
       " ('flag', 1.0): 4,\n",
       " ('display', 1.0): 9,\n",
       " ('array', 1.0): 2,\n",
       " ('pasta', 1.0): 26,\n",
       " ('festiv', 1.0): 3,\n",
       " ('turkey', 1.0): 3,\n",
       " ('ham', 1.0): 4,\n",
       " ('pie', 1.0): 6,\n",
       " ('tuesday', 1.0): 2,\n",
       " ('lack', 1.0): 59,\n",
       " ('system', 1.0): 104,\n",
       " ('congreg', 1.0): 2,\n",
       " ('group', 1.0): 36,\n",
       " ('away', 1.0): 50,\n",
       " ('menu', 1.0): 62,\n",
       " ('length', 1.0): 4,\n",
       " ('veget', 1.0): 10,\n",
       " ('tabl', 1.0): 27,\n",
       " ('practic', 1.0): 7,\n",
       " ('limit', 1.0): 73,\n",
       " ('tidi', 1.0): 12,\n",
       " ('featur', 1.0): 18,\n",
       " ('contain', 1.0): 5,\n",
       " ('page', 1.0): 2,\n",
       " ('speedbird', 1.0): 2,\n",
       " ('bacom', 1.0): 2,\n",
       " ('strategi', 1.0): 2,\n",
       " ('wither', 1.0): 1,\n",
       " ('stream', 1.0): 5,\n",
       " ('player', 1.0): 1,\n",
       " ('improv', 1.0): 106,\n",
       " ('need', 1.0): 153,\n",
       " ('face', 1.0): 46,\n",
       " ('eye', 1.0): 11,\n",
       " ('contact', 1.0): 13,\n",
       " ('uninterest', 1.0): 2,\n",
       " ('horribl', 1.0): 7,\n",
       " ('quarter', 1.0): 4,\n",
       " ('abl', 1.0): 65,\n",
       " ('beverag', 1.0): 35,\n",
       " ('proper', 1.0): 11,\n",
       " ('froth', 1.0): 1,\n",
       " ('overflow', 1.0): 1,\n",
       " ('websit', 1.0): 17,\n",
       " ('wouldnt', 1.0): 22,\n",
       " ('phone', 1.0): 15,\n",
       " ('pick', 1.0): 27,\n",
       " ('turn', 1.0): 37,\n",
       " ('two', 1.0): 153,\n",
       " ('adjac', 1.0): 6,\n",
       " ('switch', 1.0): 9,\n",
       " ('agent', 1.0): 37,\n",
       " ('fix', 1.0): 11,\n",
       " ('suggest', 1.0): 5,\n",
       " ('airsid', 1.0): 2,\n",
       " ('understaf', 1.0): 1,\n",
       " ('togeth', 1.0): 52,\n",
       " ('enthusiast', 1.0): 13,\n",
       " ('emma', 1.0): 1,\n",
       " ('daniel', 1.0): 1,\n",
       " ('obvious', 1.0): 16,\n",
       " ('learn', 1.0): 4,\n",
       " ('lesson', 1.0): 1,\n",
       " ('everyth', 1.0): 62,\n",
       " ('marrakech', 1.0): 2,\n",
       " ('post', 1.0): 7,\n",
       " ('regard', 1.0): 15,\n",
       " ('doubl', 1.0): 2,\n",
       " ('tier', 1.0): 8,\n",
       " ('confirm', 1.0): 9,\n",
       " ('promis', 1.0): 4,\n",
       " ('resolv', 1.0): 6,\n",
       " ('sinc', 1.0): 51,\n",
       " ('africa', 1.0): 12,\n",
       " ('januari', 1.0): 19,\n",
       " ('street', 1.0): 4,\n",
       " ('virtual', 1.0): 16,\n",
       " ('avio', 1.0): 53,\n",
       " ('opt', 1.0): 11,\n",
       " ('lisbon', 1.0): 15,\n",
       " ('particular', 1.0): 19,\n",
       " ('shoutout', 1.0): 1,\n",
       " ('jo', 1.0): 1,\n",
       " ('team', 1.0): 10,\n",
       " ('spend', 1.0): 14,\n",
       " ('steadi', 1.0): 1,\n",
       " ('littl', 1.0): 169,\n",
       " ('consid', 1.0): 41,\n",
       " ('weather', 1.0): 20,\n",
       " ('montreal', 1.0): 4,\n",
       " ('scotland', 1.0): 1,\n",
       " ('toronto', 1.0): 37,\n",
       " ('inflight', 1.0): 118,\n",
       " ('andthey', 1.0): 1,\n",
       " ('blanket', 1.0): 18,\n",
       " ('pillow', 1.0): 26,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement extract_features function:\n",
    "* This function takes in a single review.\n",
    "* Process the review using `process_review` function and save the list of review words.\n",
    "* Loop through each word in the list of processed words\n",
    "    * For each word, check the 'freqs' dictionary for the count when that word has a positive '1' label. (value associated with the key (word, 1.0))\n",
    "    * Do the same for the count for when the word is associated with the negative label '0'. (value associated with the key (word, 0.0).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(review, freqs, process_review=process_review):\n",
    "    '''\n",
    "    Input: \n",
    "        review: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_review tokenizes, stems, and removes stopwords\n",
    "    word_l = process_review(review)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        if (word, 1) in freqs.keys():\n",
    "            x[0,1] += freqs[(word, 1)]\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        if (word, 0) in freqs.keys():\n",
    "            x[0,2] += freqs[(word, 0)]\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    ''' \n",
    "    h = 1. / (1. + np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "#### Regression (logit)\n",
    "$$z = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n$$\n",
    "In our case $n$ is equal to $2$.\n",
    "\n",
    "#### The prediction\n",
    "$$ y_{\\text{pred}} = \\sigma(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "\n",
    "#### Binary log-loss function\n",
    "$$J(w) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (y_{\\text{pred}}^{(i)}) + (1-y^{(i)})\\log (1-y_{\\text{pred}}^{(i)})$$\n",
    "* $w = (w_0, w_1, w_2)$ is model parameters\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of training example 'i'.\n",
    "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
    "\n",
    "The goal of a training process is to minimize the loss function $J(w)$ on the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient\n",
    "$$\\dfrac{\\partial J}{\\partial w_j} (w) = \\frac{1}{m} \\sum_{i=1}^m(y_{\\text{pred}}^{(i)}-y^{(i)})x^{(i)}_j$$\n",
    "Update the weights $w$\n",
    "$$w_j = w_j - \\alpha \\dfrac{\\partial J}{\\partial w_j} (w)$$\n",
    "\n",
    "#### Vectorized formula and implementation\n",
    "* $w$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $w_0$\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix $X$ with the weight vector $w$.  $z = Xw$\n",
    "    * $X$ has dimensions (m, n+1) \n",
    "    * $w$: has dimensions (n+1, 1)\n",
    "    * $z$: has dimensions (m, 1)\n",
    "* The prediction $y_{\\text{pred}}$, is calculated by applying the sigmoid to $z$, and has dimensions (m,1).\n",
    "* The cost function $J$ in its vector form\n",
    "$$J = -\\frac{1}{m} \\times \\left(y^T \\cdot log(y_{\\text{pred}}) + (1-y)^T \\cdot log(1 - y_{\\text{pred}}) \\right)$$\n",
    "* The update of $w$ is also vectorized\n",
    "$$w = w - \\frac{\\alpha}{m} \\times \\left( X^T \\cdot \\left(y_{\\text{pred}} - y\\right) \\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    # get 'm', the number of rows in matrix X\n",
    "    m = len(x)\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = - (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h))) / float(m)\n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha * np.dot(x.T, (h-y))) / float(m)\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "* Stack the features for all training examples into a matrix X. \n",
    "* Call gradientDescent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.46042654.\n",
      "The resulting vector of weights is [1e-08, 0.00039945, -0.00036723]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6932\\2383749413.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  J = float(J)\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to predict whether a review is positive or negative.\n",
    "* Given a review, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = \\sigma(X \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 GRADED FUNCTION: predict_reviewn",
    "def predict_review(review, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        review: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(review, freqs)\n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the model precision \n",
    "* Use your 'predict_review' function to make predictions on each review in the test set.\n",
    "* If the prediction is > 0.5, set the model's classification 'y_hat' to 1, otherwise set the model's classification 'y_hat' to 0. 0.5 plays a role of the decision threshold here.\n",
    "* A prediction is accurate when the y_hat equals the test_y.  Sum up all the instances when they are equal and divide by $n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta, predict_review=predict_review):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of reviews\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of reviews\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of reviews)\n",
    "    \"\"\"   \n",
    "    # the list for storing predictions\n",
    "    y_hat = list()\n",
    "    \n",
    "    for rv in test_x:\n",
    "        # get the label prediction for the review\n",
    "        y_pred = predict_review(rv, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0.0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = np.sum(y_hat == np.squeeze(test_y)) / len(test_y)    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.7483\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excel', 'flight', 'club', 'world', 'british', 'airway', 'welcom', 'aboard', 'warm', 'continu', 'throughout', 'flight', 'crew', 'attent', 'friendli', 'profession', 'board', 'food', 'dinner', 'breakfast', 'good', 'well', 'chosen', 'select', 'wine', 'flight', 'entertain', 'offer', 'great', 'select', 'film', 'audio', 'seatflat', 'bed', 'comfort', 'british', 'airway', 'done', 'excel', 'job', 'design', 'comfort', 'suit', 'board', 'like', 'sleek', 'minimalist', 'design', 'flight', 'show', 'ba', 'among', 'worldâ€™', 'best', 'airlin']\n",
      "[[0.84529378]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Test results\n",
    "my_rv = \"An excellent flight in Club World on British Airways. The welcome aboard was warm and that continued throughout the flight. The crew were attentive, friendly and very professional. On board food for dinner and breakfast was good and there was a well chosen selection of wines. In flight entertainment offered a great selection of films and audio. The seat/flat bed was very comfortable - British Airways have done an excellent job in the design and comfort of the suites on board the A350. I liked the sleek, minimalist design. This flight showed that BA can be among the worldâ€™s best airlines.\"\n",
    "print(process_review(my_rv))\n",
    "y_hat = predict_review(my_rv, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
